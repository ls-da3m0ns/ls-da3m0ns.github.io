<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-06-25T09:10:28+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Real Developers</title><subtitle>Exprience the fun of explore and learning</subtitle><author><name>Prashant Singh</name></author><entry><title type="html">Tensorflow TPU and TFrecords</title><link href="http://localhost:4000/linux/ml/Tensorflow_TPU_and_Tfrecords" rel="alternate" type="text/html" title="Tensorflow TPU and TFrecords" /><published>2021-01-27T00:00:00+05:30</published><updated>2021-01-27T00:00:00+05:30</updated><id>http://localhost:4000/linux/ml/Tensorflow_TPU_and_Tfrecords</id><content type="html" xml:base="http://localhost:4000/linux/ml/Tensorflow_TPU_and_Tfrecords">&lt;p&gt;Before I start let me first say this “TPUs are fast and with fast i mean crazy fast the biggest bottleneck for TPU is its data loading process”&lt;/p&gt;

&lt;p&gt;This article will be focused on how to combine TFrecod format with TPU processing to optimize data loading and minimize the training time&lt;/p&gt;

&lt;h2 id=&quot;there-are-five-major-steps-to-keep-in-mind-when-you-are-using-tpu-in-tensorflow&quot;&gt;There are Five major steps to keep in mind when you are using TPU in Tensorflow&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;tpu-initialization&quot;&gt;TPU initialization&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;distribution-strategy&quot;&gt;Distribution Strategy&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;data-type&quot;&gt;Data Type&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;data-loading&quot;&gt;Data loading&lt;/h4&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h4 id=&quot;training&quot;&gt;Training&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tpu-initialization-1&quot;&gt;TPU initialization&lt;/h2&gt;

&lt;p&gt;Lets Start with TPU initialization, Its a very simple process and very important one the reason for this step is because TPU’s are multi node workers in the Cloud you dont have 
direct access to them like you have with GPU’s and CPU’s so you need to initialize network connection to each TPU processing node.&lt;/p&gt;

&lt;p&gt;Below is an Example code to do so&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;distribution-strategy-1&quot;&gt;Distribution Strategy&lt;/h2&gt;

&lt;p&gt;Now as there are multiple devices to work with you need some distribution strategy so that you can take advantage of multiple processing nodes the concept of distribution strategy is similar to what you might use for multi GPU training&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/guide/distributed_training&quot;&gt;Read More about distributed training here&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# instantiate a distribution strategy
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)
AUTO = tf.data.experimental.AUTOTUNE
REPLICAS = tpu_strategy.num_replicas_in_sync
print(f'REPLICAS: {REPLICAS}')

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;data-type-1&quot;&gt;Data Type&lt;/h2&gt;
&lt;p&gt;Currently, only the tf.float32, tf.int32, tf.bfloat16, and tf.bool data types are supported on the TPU. Other common data types, such as tf.uint8, tf.string, and tf.int64, must be converted to one of the supported data types during data pre-processing. I mostly use float32 and personally never tried bfloat32 before but it should work just fine.&lt;/p&gt;

&lt;h2 id=&quot;data-loading-1&quot;&gt;Data Loading&lt;/h2&gt;
&lt;p&gt;This is by far the most important step in TPU training correct implementation can give you huge perfomance boost.&lt;/p&gt;

&lt;h3 id=&quot;tensorflow-and-tpus-only-work-with-data-hosted-on-gcs-atleast-for-now&quot;&gt;Tensorflow and TPU’s only work with data hosted on GCS atleast for now&lt;/h3&gt;

&lt;p&gt;If you are using existing dataset from tensorflow dataset use &lt;code class=&quot;highlighter-rouge&quot;&gt;try_gcs=True&lt;/code&gt; flag to load data from GCS other wise my recomendation is to host your data in tfrecord format in GCS buckets 
I personally use Kaggle to host dataset and get GCS path from kaggle kernel that’s just me you can always use your personal GCP account&lt;/p&gt;

&lt;p&gt;below is the example how i loaded Ranzcer competition dataset from gcs in tfrecord format&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;overview of code structure&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;select tfrecord files to be used for data loading in my case i had 15 tfrecord files
load it using tf.data.TFRecordDataset api 
then map functions to decode labels and images and then convert it to particular data type 
finally before training configure perfomance optimizing and neccesary steps for example batch,prefetch,shuffle,repeat etc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def to_float32_2(image, label):
    max_val = tf.reduce_max(label, axis=-1,keepdims=True)
    cond = tf.equal(label, max_val)
    label = tf.where(cond, tf.ones_like(label), tf.zeros_like(label))
    return tf.cast(image, tf.float32), tf.cast(label, tf.int32)

def to_float32(image, label):
    return tf.cast(image, tf.float32), label

def decode_image(image_data):
    image = tf.image.decode_jpeg(image_data, channels=3)
    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range
    image = tf.reshape(image, [1024,1024, 3]) # explicit size needed for TPU
    return image


def read_labeled_tfrecord(example):
    # Create a dictionary describing the features.
    LABELED_TFREC_FORMAT = {
        &quot;StudyInstanceUID&quot;           : tf.io.FixedLenFeature([], tf.string),
        &quot;image&quot;                      : tf.io.FixedLenFeature([], tf.string),
        &quot;ETT - Abnormal&quot;             : tf.io.FixedLenFeature([], tf.int64), 
        &quot;ETT - Borderline&quot;           : tf.io.FixedLenFeature([], tf.int64), 
        &quot;ETT - Normal&quot;               : tf.io.FixedLenFeature([], tf.int64), 
        &quot;NGT - Abnormal&quot;             : tf.io.FixedLenFeature([], tf.int64), 
        &quot;NGT - Borderline&quot;           : tf.io.FixedLenFeature([], tf.int64), 
        &quot;NGT - Incompletely Imaged&quot;  : tf.io.FixedLenFeature([], tf.int64), 
        &quot;NGT - Normal&quot;               : tf.io.FixedLenFeature([], tf.int64), 
        &quot;CVC - Abnormal&quot;             : tf.io.FixedLenFeature([], tf.int64), 
        &quot;CVC - Borderline&quot;           : tf.io.FixedLenFeature([], tf.int64), 
        &quot;CVC - Normal&quot;               : tf.io.FixedLenFeature([], tf.int64), 
        &quot;Swan Ganz Catheter Present&quot; : tf.io.FixedLenFeature([], tf.int64),
    }
    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)
    image = decode_image(example['image']) 
    image= tf.image.resize(image, [IMAGE_SIZE[0],IMAGE_SIZE[0]])
    uid= example[&quot;StudyInstanceUID&quot;]
    cvca = example[&quot;CVC - Abnormal&quot;]
    cvcb = example[&quot;CVC - Borderline&quot;]
    cvcn = example[&quot;CVC - Normal&quot;]
    etta = example[&quot;ETT - Abnormal&quot;]
    ettb = example[&quot;ETT - Borderline&quot;]
    ettn = example[&quot;ETT - Normal&quot;]
    ngta = example[&quot;NGT - Abnormal&quot;]
    ngtb = example[&quot;NGT - Borderline&quot;]
    ngti = example[&quot;NGT - Incompletely Imaged&quot;]
    ngtn = example[&quot;NGT - Normal&quot;]
    sgcp = example[&quot;Swan Ganz Catheter Present&quot;]

    label  = [etta, ettb, ettn, ngta, ngtb, ngti, ngtn,cvca, cvcb, cvcn , sgcp]
    label=[tf.cast(i,tf.float32) for i in label]
    return image,label # returns a dataset of (image, label) pairs

def read_unlabeled_tfrecord(example):
    UNLABELED_TFREC_FORMAT  = {
    &quot;StudyInstanceUID&quot; : tf.io.FixedLenFeature([], tf.string),
    &quot;image&quot; : tf.io.FixedLenFeature([], tf.string)
    }
    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)
    image = decode_image(example['image'])
    image= tf.image.resize(image, [IMAGE_SIZE[0],IMAGE_SIZE[0]])
    image_name = example['StudyInstanceUID']
    return image, image_name # returns a dataset of image(s)

def read_labeled_tf_record(filenames, labeled=True, ordered=False):
    ignore_order = tf.data.Options()
    if not ordered:
        ignore_order.experimental_deterministic = False # disable order, increase speed

    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files
    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order
    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)
    return dataset

def data_augment(image, label):
    image = tf.image.random_flip_left_right(image , seed=SEED)
    image = tf.image.random_flip_up_down(image, seed=SEED)
    image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness
    image = tf.image.random_saturation(image, 0, 2, seed=SEED)
    image = tf.image.adjust_saturation(image, 3)
    
    #image = tf.image.central_crop(image, central_fraction=0.5)
    return image, label   

def get_training_dataset(dataset):
    dataset = dataset.shuffle(2048)
    dataset = dataset.repeat() # the training dataset must repeat for several epochs
    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)
    return dataset

def get_validation_dataset(ordered=False):
    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True, ordered=ordered)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.cache()
    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)
    return dataset

def get_test_dataset(ordered=False):
    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)
    return dataset

def load_dataset(filenames, labeled = True, ordered = False):
    ignore_order = tf.data.Options()
    
    if not ordered:
        ignore_order.experimental_deterministic = False
    
    dataset = (tf.data.TFRecordDataset(filenames, num_parallel_reads = AUTO).with_options(ignore_order).
               map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls = AUTO))
    
    return dataset

training_filenames = []
training_filenames.append(GCS_DS_PATH + '/train_tfrecords/*.tfrec')
TRAINING_FILENAMES = tf.io.gfile.glob(training_filenames)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;training-1&quot;&gt;Training&lt;/h2&gt;

&lt;p&gt;This is fairly easy stuff all you have do is to make sure what ever model or layers you are loading are hosted in GCS and model compilation should happen within the 
scope of distributed strategy&lt;/p&gt;

&lt;p&gt;Example structure of code&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;load_all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;your&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt; 
    &lt;span class=&quot;nf&quot;&gt;compile&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;here&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tpu_strategy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thats It now you know how to make use of TPU’s along with TFRecord format to get faster training&lt;/p&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="ML" /><summary type="html">Before I start let me first say this “TPUs are fast and with fast i mean crazy fast the biggest bottleneck for TPU is its data loading process”</summary></entry><entry><title type="html">Kubernetes Introduction</title><link href="http://localhost:4000/linux/kubernetes-introduction" rel="alternate" type="text/html" title="Kubernetes Introduction" /><published>2021-01-14T00:00:00+05:30</published><updated>2021-01-14T00:00:00+05:30</updated><id>http://localhost:4000/linux/kubernetes-introduction</id><content type="html" xml:base="http://localhost:4000/linux/kubernetes-introduction">&lt;p&gt;With this article I am starting a new series of blog about kubernetes I will be going in detail about kubernetes and will also deploy lots of projects and demos
all the theoretical knowledge will be followed by sample code.&lt;/p&gt;

&lt;p&gt;So Lets Start&lt;/p&gt;

&lt;p&gt;&lt;b&gt;What the heck is kubernetes?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Before we understand kubernetes we should first ask &lt;u&gt;why its is used&lt;/u&gt; and the answer to that in simple words is to manage containers.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;But what is this container ?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Containers are similar to Virtual Machines (VMs) but not as isolated like VMs while VMs enjoy compelete isolation from base OS
containers use base OS to make themselves lightweight but only to a extent where they are still potable.
Similar to VMs Containers have there own filesystem, share of CPU, memory, process space.&lt;/p&gt;

&lt;p&gt;&lt;u&gt;So in lot more simple way containers give you a full fledge VM like environment but faster and less resource hungry.&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;There are lots of container services but one of them which is famous for many good reasons is Docker&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Now lets comeback to our main question what is kubernetes?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Although containers are great way to deploy your application but in production you need a way to easily manage these containers and ensure there is no downtime 
this is where services like kubernetes and other container-Orchestration tools comes in Kubernetes provides you with a framework to run distributed systems resiliently
in simpler words It takes care of scaling and failover for your application and can even help you with your deployment pattern.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Services Provided By Kubernetes : &lt;/b&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Service discovery and load balancing
Storage Orchestration 
Automated rollouts and rollbacks
Automatic bin packing
self healing
Secret and configuration Management
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Don’t worry if you dont understand any of the above services we will learn each of them in detail in comming articles&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Myth regarding kubernetes &lt;/b&gt;
It is a PaaS&lt;/p&gt;

&lt;p&gt;this is one of the biggest myth in many users mind that kubernetes can operate on hardware level but thats not true it operates on container level 
But still it can provide some of the common PaaS offerings Such as Load Balancing, Deployment, Scaling, logging, monitoring etc&lt;/p&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="Linux" /><summary type="html">With this article I am starting a new series of blog about kubernetes I will be going in detail about kubernetes and will also deploy lots of projects and demos all the theoretical knowledge will be followed by sample code.</summary></entry><entry><title type="html">Solar Radiation Prediction</title><link href="http://localhost:4000/linux/ml/solar-radiation-prediction" rel="alternate" type="text/html" title="Solar Radiation Prediction" /><published>2020-12-19T00:00:00+05:30</published><updated>2020-12-19T00:00:00+05:30</updated><id>http://localhost:4000/linux/ml/solar-radiation-prediction</id><content type="html" xml:base="http://localhost:4000/linux/ml/solar-radiation-prediction">&lt;p&gt;This is my approach to Solar Radiation Prediction Challenge hosted on Dockship, My main focus was on EDA part rather than modeling but still a very basic approach got me below 18 RMSE on testing data that’s pretty awesome.&lt;/p&gt;

&lt;h2 id=&quot;this-might-take-a-few-seconds-to-load&quot;&gt;This might take a few seconds to load&lt;/h2&gt;

&lt;script src=&quot;https://gist.github.com/eb3b23c5f9dc34962b0724ee6bd81808.js&quot;&gt; &lt;/script&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="ML" /><summary type="html">This is my approach to Solar Radiation Prediction Challenge hosted on Dockship, My main focus was on EDA part rather than modeling but still a very basic approach got me below 18 RMSE on testing data that’s pretty awesome.</summary></entry><entry><title type="html">Top 0.1% EDA for metro-interstate-traffic-volume</title><link href="http://localhost:4000/linux/ml/traffic_volume_prediction" rel="alternate" type="text/html" title="Top 0.1% EDA for metro-interstate-traffic-volume" /><published>2020-12-18T00:00:00+05:30</published><updated>2020-12-18T00:00:00+05:30</updated><id>http://localhost:4000/linux/ml/traffic_volume_prediction</id><content type="html" xml:base="http://localhost:4000/linux/ml/traffic_volume_prediction">&lt;p&gt;This is my approach to metro-interstate-traffic-volume major feature of this EDA is its Data Cleaning step while Modeling Step was pretty Straight forward and think this is where you guys can 
improve and push the RMSE a bit lower.&lt;/p&gt;

&lt;h2 id=&quot;this-might-take-a-few-seconds-to-load&quot;&gt;This might take a few seconds to load&lt;/h2&gt;

&lt;script src=&quot;https://gist.github.com/f71d45feb9c81ac63b53f813564c279e.js&quot;&gt; &lt;/script&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="ML" /><summary type="html">This is my approach to metro-interstate-traffic-volume major feature of this EDA is its Data Cleaning step while Modeling Step was pretty Straight forward and think this is where you guys can improve and push the RMSE a bit lower.</summary></entry><entry><title type="html">100% accuracy on Fake News Classifiaction problem</title><link href="http://localhost:4000/linux/ml/fake-news-detector" rel="alternate" type="text/html" title="100% accuracy on Fake News Classifiaction problem" /><published>2020-12-18T00:00:00+05:30</published><updated>2020-12-18T00:00:00+05:30</updated><id>http://localhost:4000/linux/ml/fake-news-detector</id><content type="html" xml:base="http://localhost:4000/linux/ml/fake-news-detector">&lt;p&gt;This is my approach to fake news classification challenge, If You ask me this was prettyu basic competion the trick was one of the test case ehich had no content so as usual my approach was to get as much content so i merged title and content column and that got me to 100% accuracy.&lt;/p&gt;

&lt;h2 id=&quot;this-might-take-a-few-seconds-to-load&quot;&gt;This might take a few seconds to load&lt;/h2&gt;

&lt;script src=&quot;https://gist.github.com/e7f3a962817e027630e9a7e7a066eacf.js&quot;&gt; &lt;/script&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="ML" /><summary type="html">This is my approach to fake news classification challenge, If You ask me this was prettyu basic competion the trick was one of the test case ehich had no content so as usual my approach was to get as much content so i merged title and content column and that got me to 100% accuracy.</summary></entry><entry><title type="html">Pytorch Lightning Transfer Learning on custom dataset</title><link href="http://localhost:4000/linux/ml/pytorchlightning" rel="alternate" type="text/html" title="Pytorch Lightning Transfer Learning on custom dataset" /><published>2020-12-04T00:00:00+05:30</published><updated>2020-12-04T00:00:00+05:30</updated><id>http://localhost:4000/linux/ml/pytorchlightning</id><content type="html" xml:base="http://localhost:4000/linux/ml/pytorchlightning">&lt;p&gt;&lt;b&gt;Why use PyTorch Lightning when you already have PyTorch ?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Although PyTorch is great but when task becomes complex there’s lots of small mistakes that can happen this is where PyTorch Lightning shines it structures your training and preparation such thatits both extensible for advance users and easy to use for beginners&lt;/p&gt;

&lt;p&gt;For this Blog we will be using Butterfly Dataset which contains images of 50 different classes of butterfly.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Imports&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;This is pretty straight forward and dose not require much explanation&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_8/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;&lt;b&gt;Dataset &lt;/b&gt;&lt;/p&gt;

&lt;p&gt;PyTorch Lightning has a clean way of handling data using classes, it has pre-built  hooks which automatically get attached to the required method of the class and also are customizable.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_8/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Few things to note here prepare_data function is called only once during training while function setup is called once for each device in the cluster.&lt;/p&gt;

&lt;p&gt;Lets say you have 8 cores in a TPU then prepare_data would be called once (generally for downloading data ) then setup would be called once for each 8 cores&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Model&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;This is where most of the PyTorch lightning work is done, PyTorch lightning has preconfigured hooks that allows us train model carefree for example it automatically save checkpoint after each epoch, implements early_stopping if loss metrics is available and automatically setups device for you this allows us to run same code on CPU,GPU and also TPU&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_8/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;Here we are using ResNet50 for 50 classes and Adam optimizer with fixed learning rate of 1e-4&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Training&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Training is as simple as calling trainer.fit in PyTorch Lightning&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_8/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="Linux" /><category term="ML" /><summary type="html">Why use PyTorch Lightning when you already have PyTorch ?</summary></entry><entry><title type="html">What are L1,L2 and L3 cache</title><link href="http://localhost:4000/linux/l1l2l3" rel="alternate" type="text/html" title="What are L1,L2 and L3 cache" /><published>2020-12-03T00:00:00+05:30</published><updated>2020-12-03T00:00:00+05:30</updated><id>http://localhost:4000/linux/l1l2l3</id><content type="html" xml:base="http://localhost:4000/linux/l1l2l3">&lt;p&gt;Current generation Computer processors have processing speed of around 3Ghz, and while even high end RAM have speed of 2.4Ghz. As you can see this could be a huge bottle neck for Computers this problem is resolved by using something called CPU cache.&lt;/p&gt;

&lt;p&gt;In Computers mainly there are two type of memory&lt;/p&gt;

&lt;p&gt;&lt;b&gt;DRAM (dynamic ram )&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;These use capacitors and because of that they need to be constantly refreshed using electricity. DRAMs are generally slower than SRAM. A great example of DRAM is RAM.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;SRAM (static ram)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;SRAM uses flip flop thus having greater speed and does not need to be refreshed constantly, but SRAM are more expensive than DRAM. They are generally used in CPU cache.&lt;/p&gt;

&lt;p&gt;CPU cache is also called internal memory of CPU, it stores copies of data and instruction from RAM, CPU constantly needs files from RAM CPU cache acts as middle man and stores most frequently requested files by CPU thus increasing efficiency of processor.&lt;/p&gt;

&lt;p&gt;There are three types of CPU cache L1(further split into L1d (data) and L1i (instruction) ) L2 and L3&lt;/p&gt;

&lt;p&gt;In Linux based OS we can use lscpu command to get capacity of each type of cache&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_5/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;&lt;b&gt;L1 cache&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;It is located on the processor and every processor has its own L1 cache. L1 cache is the fastest cache in the computer and runs at the same speed as that of the processor.&lt;/p&gt;

&lt;p&gt;L1 cache is nowadays splitted between L1d (Data) and L1i (instruction)&lt;/p&gt;

&lt;p&gt;&lt;b&gt;L2 cache&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;L2 cache is also known as external cache, as the name suggests it is located outside the processor (core). It stores recent data accessed by CPU which are not stored in L1 cache. If the CPU can’t find data in L1 cache it looks in L2 cache.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;L3 cache&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;L3 cache is also called shared cache as it is shared between all the processors. It stores recent data accessed by CPU which is not available in both L1 or L2 cache . If CPU doesn’t finds data here then it will have to request data from much slower memory RAM&lt;/p&gt;

&lt;p&gt;&lt;b&gt;How cache stores data&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;In every program, there are usually a few instructions that repeat again and again, whereas the rest are executed only once or twice. Those repeating part of code is sent straight to cache, But these codes lines are not consecutive so a “label” is assigned to each cache position, the label is equal to the position of that code line in the ROM. Hence when CPU ask for a code line&lt;/p&gt;

&lt;p&gt;first requested ROM possition is compared with all the labels in the cache. If there is a match (also called cache hit) then associated code line from cache is extracted.&lt;/p&gt;

&lt;p&gt;If there is no match (also called cache miss ) code is extracted from ROM, naturally in this case time taken will be more than usual.&lt;/p&gt;

&lt;p&gt;also a new entry is created in cache for each miss, and to create this space least-recently used cache entry is deleted&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Why CPU cache is faster?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Along side each cached line of memory there are extra memory cells that store part(or all ) of the address. therefore all the cells can be queried to see whether they have the particular line of memory that cpu wants and then data will dump it onto bus that connects the main memory to the processor core. this happens in less than a cycle, because it is much simpler&lt;/p&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="Linux" /><summary type="html">Current generation Computer processors have processing speed of around 3Ghz, and while even high end RAM have speed of 2.4Ghz. As you can see this could be a huge bottle neck for Computers this problem is resolved by using something called CPU cache.</summary></entry><entry><title type="html">Use of O_SYNC flag while opening file</title><link href="http://localhost:4000/linux/osync" rel="alternate" type="text/html" title="Use of O_SYNC flag while opening file" /><published>2020-12-03T00:00:00+05:30</published><updated>2020-12-03T00:00:00+05:30</updated><id>http://localhost:4000/linux/osync</id><content type="html" xml:base="http://localhost:4000/linux/osync">&lt;p&gt;Opening a file in Linux is generally done by the help of open( ) system call. The open( ) function establishes the connection between a file and a file descriptor. It will create an open file description that refers to a file and a file descriptor that refers to that open file description. The file descriptor is used by other I/O functions to refer to the file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int open(const char *pathname, int flags, mode_t mode);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The function open( ) opens the file specified by pathname. If the specified file does not exist it may optionally be created by providing the&lt;b&gt; O_CREAT&lt;/b&gt; flag in open( ). The argument flags must include one of the following access modes: &lt;b&gt;O_RDONLY&lt;/b&gt;, &lt;b&gt;O_WRONLY&lt;/b&gt;,&lt;b&gt; or O_RDWR &lt;/b&gt;. These requests open the file in read-only, write-only, or read/write modes, respectively.&lt;/p&gt;

&lt;p&gt;In addition, zero or more file creation flags and file status flags can be bitwise-or%u2019d in flags. The file creation flags affect the semantics of the open operation itself while the file status flag affects the semantics of subsequent I/O operations.&lt;/p&gt;

&lt;p&gt;&lt;b&gt; Bitwise-or of O_RDONLY and O_SYNC flag:&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Synchronized I/O specifies the open() flags O_SYNC,O_DSYNC, and O_RSYNC for controlling the behaviour. Regardless of whether an implementation supports this option, it must at least support the use of O_SYSNC for regular files.&lt;/p&gt;

&lt;p&gt;Linux implements O_SYNC and O_DSYNC, but not O_RSYNC. Somewhat incorrectly, glibc defines O_RSYNC to have the same value as O_SYNC.&lt;/p&gt;

&lt;p&gt;O_SYNC provides synchronized I/O file integrity completion, meaning write operations will flush data and all associated metadata to the underlying hardware.&lt;/p&gt;

&lt;p&gt;O_DSYNC provides synchronized I/O data integrity completion, meaning write operations will flush data to the underlying hardware, but will only flush metadata updates that are required to allow a subsequent read operation to complete successfully.&lt;/p&gt;

&lt;p&gt;Data integrity completion can reduce the number of disk operations that are required for applications that don’t need the guarantees of file integrity completion.&lt;/p&gt;

&lt;p&gt;Since Linux 2.6.33, proper O_SYNC support has been provided. However, to ensure backward binary compatibility, O_DSYNC was defined with the same value as the historical O_SYNC, and O_SYNC was defined as a new (two-bit) flag value that includes the O_DSYNC flag value.&lt;/p&gt;

&lt;p&gt;This ensures that applications compiled against new headers get at least O_DSYNC semantics on pre-2.6.33 kernels.&lt;/p&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="Linux" /><summary type="html">Opening a file in Linux is generally done by the help of open( ) system call. The open( ) function establishes the connection between a file and a file descriptor. It will create an open file description that refers to a file and a file descriptor that refers to that open file description. The file descriptor is used by other I/O functions to refer to the file.</summary></entry><entry><title type="html">Why running ps command without options on shell shows only two entries</title><link href="http://localhost:4000/linux/pscommand" rel="alternate" type="text/html" title="Why running ps command without options on shell shows only two entries" /><published>2020-12-03T00:00:00+05:30</published><updated>2020-12-03T00:00:00+05:30</updated><id>http://localhost:4000/linux/pscommand</id><content type="html" xml:base="http://localhost:4000/linux/pscommand">&lt;p&gt;&lt;b&gt; What is ps command?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;ps also known as “Process status” is a command in linux which provides information about the current running process. ps command accepts several kinds of options like -&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;UNIX option : which are always preceded by dash

BSD options : which are not preceded by dash

GNU options : which are preceded by double dash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;b&gt;Why does ps only show 2 items?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;So when you run only “ps” command it only shows the process associated with the current terminal and with the same effective user id (euid) as that of the current user. In most cases it’s only two processes that run with that config but more processes can be created and displayed in ps command output.&lt;/p&gt;

&lt;p&gt;To verify the same we can run a ping command from the current user and terminal then see the output of the “ps”, we will also verify the parent pid of the process.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_7/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;As you can see bash is the parent of both the process and the user is home.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Common ways of using ps command&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;ps can be paired with many arguments but some of them are more used than others those are&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ps -aux
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is perhaps the most widely used configuration of ps command as it lists detailed info of all the process of all the users on a system. Apart from the process associated with the terminal it also lists demons running in the background.&lt;/p&gt;

&lt;p&gt;It can then be piped into other commands for more formatted and specific info like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ps -aux | less
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_7/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ps -elf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is an alternative for viewing every process running in a system. It creates a table with 15 column.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_7/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;&lt;b&gt;From where does the “ps” command get all the info?&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;ps command does not need any special permission like sudo because it reads the files present in /proc directory. The /proc directory is a special directory as it is the mount point of the proc filesystem which contains virtual files representing each process in the system with their pid as name.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_7/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><summary type="html">What is ps command?</summary></entry><entry><title type="html">Deep Learning Approach in detecting Malware (STAMINA)</title><link href="http://localhost:4000/linux/ml/stamina" rel="alternate" type="text/html" title="Deep Learning Approach in detecting Malware (STAMINA)" /><published>2020-12-03T00:00:00+05:30</published><updated>2020-12-03T00:00:00+05:30</updated><id>http://localhost:4000/linux/ml/stamina</id><content type="html" xml:base="http://localhost:4000/linux/ml/stamina">&lt;p&gt;Intel Labs and Microsoft Threat Protection Intelligence Teams are collaborating to research the application of deep learning for malware threat detection. Intel and Microsoft have previously demonstrated that transfer learning from computer vision for malware analysis can achieve highly desirable classification performance.&lt;/p&gt;

&lt;p&gt;The companies call the project STAMINA. The main aim of STAMINA (STAtic Malware-as-image Network Analysis) is to Leverage Deep learning techniques to avoid time-consuming manual feature engineering with high accuracy and low false positives.&lt;/p&gt;

&lt;p&gt;Static analysis is a quick and straightforward way to detect malware without executing the application or monitoring the run time behaviour, static analysis technique is used to match malicious signatures.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_4/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;&lt;b&gt;Preprocessing (image conversion)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;An image is obtained from a binary application by assigning a value between 0–255 to every byte, which directly corresponds to pixel intensity. The resulting1-D pixel stream is then converted to 2-D with the help of a table shown below that gives width according to file size, height is obtained by no of pixels divided by width. After reshaping images are resized to 224 or 299 using bilinear interpolation or nearest neighbor algorithms.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_4/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;&lt;b&gt;Transfer learning Step&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;Due to limitation of datasets training a complete deep neural network can be difficult therefore transfer learning is used. The idea here is to borrow knowledge learned from a model used in one domain and apply it to another target domain.&lt;/p&gt;

&lt;p&gt;A portion of layers are frozen and the last few layers are fine-tuned on a newly obtained dataset.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/posts/blog_4/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;This approach of malware classification resulted in accuracy upto 99% with 2.6% false positive rating. While this technique is revolutionary it is still in early stages. It’s extremely effective in analyzing small files but with large binaries STAMINA lags . The joint research encourages the use of deep transfer learning for the purpose of malware classification.&lt;/p&gt;</content><author><name>Prashant Singh</name></author><category term="linux" /><category term="ML" /><summary type="html">Intel Labs and Microsoft Threat Protection Intelligence Teams are collaborating to research the application of deep learning for malware threat detection. Intel and Microsoft have previously demonstrated that transfer learning from computer vision for malware analysis can achieve highly desirable classification performance.</summary></entry></feed>